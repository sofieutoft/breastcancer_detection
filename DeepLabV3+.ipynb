{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e62528c",
   "metadata": {},
   "source": [
    "### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f22851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: albumentations in ./.venv/lib/python3.13/site-packages (2.0.5)\n",
      "Requirement already satisfied: segmentation-models-pytorch in ./.venv/lib/python3.13/site-packages (0.5.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (79.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.13/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./.venv/lib/python3.13/site-packages (from albumentations) (1.15.2)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.13/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in ./.venv/lib/python3.13/site-packages (from albumentations) (2.11.3)\n",
      "Requirement already satisfied: albucore==0.0.23 in ./.venv/lib/python3.13/site-packages (from albumentations) (0.0.23)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in ./.venv/lib/python3.13/site-packages (from albumentations) (4.11.0.86)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in ./.venv/lib/python3.13/site-packages (from albucore==0.0.23->albumentations) (3.12.5)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in ./.venv/lib/python3.13/site-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in ./.venv/lib/python3.13/site-packages (from segmentation-models-pytorch) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.13/site-packages (from segmentation-models-pytorch) (0.5.3)\n",
      "Requirement already satisfied: timm>=0.9 in ./.venv/lib/python3.13/site-packages (from segmentation-models-pytorch) (1.0.15)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.13/site-packages (from segmentation-models-pytorch) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision albumentations segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e66986b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f051cf",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a0b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiautoft/classes/DS/liver_segmentation/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "Path to dataset files: /Users/sofiautoft/.cache/kagglehub/datasets/aryashah2k/breast-ultrasound-images-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"aryashah2k/breast-ultrasound-images-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bae27",
   "metadata": {},
   "source": [
    "### Load & Resize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: benign (1).png and benign (1)_mask.png\n",
      "Loading: benign (10).png and benign (10)_mask.png\n",
      "Loading: benign (100).png and benign (100)_mask.png\n",
      "Loading: benign (101).png and benign (100)_mask_1.png\n",
      "Loading: benign (102).png and benign (101)_mask.png\n",
      "Loading: benign (103).png and benign (102)_mask.png\n",
      "Loading: benign (104).png and benign (103)_mask.png\n",
      "Loading: benign (105).png and benign (104)_mask.png\n",
      "Loading: benign (106).png and benign (105)_mask.png\n",
      "Loading: benign (107).png and benign (106)_mask.png\n",
      "Loading: benign (108).png and benign (107)_mask.png\n",
      "Loading: benign (109).png and benign (108)_mask.png\n",
      "Loading: benign (11).png and benign (109)_mask.png\n",
      "Loading: benign (110).png and benign (11)_mask.png\n",
      "Loading: benign (111).png and benign (110)_mask.png\n",
      "Loading: benign (112).png and benign (111)_mask.png\n",
      "Loading: benign (113).png and benign (112)_mask.png\n",
      "Loading: benign (114).png and benign (113)_mask.png\n",
      "Loading: benign (115).png and benign (114)_mask.png\n",
      "Loading: benign (116).png and benign (115)_mask.png\n",
      "Loading: benign (117).png and benign (116)_mask.png\n",
      "Loading: benign (118).png and benign (117)_mask.png\n",
      "Loading: benign (119).png and benign (118)_mask.png\n",
      "Loading: benign (12).png and benign (119)_mask.png\n",
      "Loading: benign (120).png and benign (12)_mask.png\n",
      "Loading: benign (121).png and benign (120)_mask.png\n",
      "Loading: benign (122).png and benign (121)_mask.png\n",
      "Loading: benign (123).png and benign (122)_mask.png\n",
      "Loading: benign (124).png and benign (123)_mask.png\n",
      "Loading: benign (125).png and benign (124)_mask.png\n",
      "Loading: benign (126).png and benign (125)_mask.png\n",
      "Loading: benign (127).png and benign (126)_mask.png\n",
      "Loading: benign (128).png and benign (127)_mask.png\n",
      "Loading: benign (129).png and benign (128)_mask.png\n",
      "Loading: benign (13).png and benign (129)_mask.png\n",
      "Loading: benign (130).png and benign (13)_mask.png\n",
      "Loading: benign (131).png and benign (130)_mask.png\n",
      "Loading: benign (132).png and benign (131)_mask.png\n",
      "Loading: benign (133).png and benign (132)_mask.png\n",
      "Loading: benign (134).png and benign (133)_mask.png\n",
      "Loading: benign (135).png and benign (134)_mask.png\n",
      "Loading: benign (136).png and benign (135)_mask.png\n",
      "Loading: benign (137).png and benign (136)_mask.png\n",
      "Loading: benign (138).png and benign (137)_mask.png\n",
      "Loading: benign (139).png and benign (138)_mask.png\n",
      "Loading: benign (14).png and benign (139)_mask.png\n",
      "Loading: benign (140).png and benign (14)_mask.png\n",
      "Loading: benign (141).png and benign (140)_mask.png\n",
      "Loading: benign (142).png and benign (141)_mask.png\n",
      "Loading: benign (143).png and benign (142)_mask.png\n",
      "Loading: benign (144).png and benign (143)_mask.png\n",
      "Loading: benign (145).png and benign (144)_mask.png\n",
      "Loading: benign (146).png and benign (145)_mask.png\n",
      "Loading: benign (147).png and benign (146)_mask.png\n",
      "Loading: benign (148).png and benign (147)_mask.png\n",
      "Loading: benign (149).png and benign (148)_mask.png\n",
      "Loading: benign (15).png and benign (149)_mask.png\n",
      "Loading: benign (150).png and benign (15)_mask.png\n",
      "Loading: benign (151).png and benign (150)_mask.png\n",
      "Loading: benign (152).png and benign (151)_mask.png\n",
      "Loading: benign (153).png and benign (152)_mask.png\n",
      "Loading: benign (154).png and benign (153)_mask.png\n",
      "Loading: benign (155).png and benign (154)_mask.png\n",
      "Loading: benign (156).png and benign (155)_mask.png\n",
      "Loading: benign (157).png and benign (156)_mask.png\n",
      "Loading: benign (158).png and benign (157)_mask.png\n",
      "Loading: benign (159).png and benign (158)_mask.png\n",
      "Loading: benign (16).png and benign (159)_mask.png\n",
      "Loading: benign (160).png and benign (16)_mask.png\n",
      "Loading: benign (161).png and benign (160)_mask.png\n",
      "Loading: benign (162).png and benign (161)_mask.png\n",
      "Loading: benign (163).png and benign (162)_mask.png\n",
      "Loading: benign (164).png and benign (163)_mask.png\n",
      "Loading: benign (165).png and benign (163)_mask_1.png\n",
      "Loading: benign (166).png and benign (164)_mask.png\n",
      "Loading: benign (167).png and benign (165)_mask.png\n",
      "Loading: benign (168).png and benign (166)_mask.png\n",
      "Loading: benign (169).png and benign (167)_mask.png\n",
      "Loading: benign (17).png and benign (168)_mask.png\n",
      "Loading: benign (170).png and benign (169)_mask.png\n",
      "Loading: benign (171).png and benign (17)_mask.png\n",
      "Loading: benign (172).png and benign (170)_mask.png\n",
      "Loading: benign (173).png and benign (171)_mask.png\n",
      "Loading: benign (174).png and benign (172)_mask.png\n",
      "Loading: benign (175).png and benign (173)_mask.png\n",
      "Loading: benign (176).png and benign (173)_mask_1.png\n",
      "Loading: benign (177).png and benign (174)_mask.png\n",
      "Loading: benign (178).png and benign (175)_mask.png\n",
      "Loading: benign (179).png and benign (176)_mask.png\n",
      "Loading: benign (18).png and benign (177)_mask.png\n",
      "Loading: benign (180).png and benign (178)_mask.png\n",
      "Loading: benign (181).png and benign (179)_mask.png\n",
      "Loading: benign (182).png and benign (18)_mask.png\n",
      "Loading: benign (183).png and benign (180)_mask.png\n",
      "Loading: benign (184).png and benign (181)_mask.png\n",
      "Loading: benign (185).png and benign (181)_mask_1.png\n",
      "Loading: benign (186).png and benign (182)_mask.png\n",
      "Loading: benign (187).png and benign (183)_mask.png\n",
      "Loading: benign (188).png and benign (184)_mask.png\n",
      "Loading: benign (189).png and benign (185)_mask.png\n",
      "Loading: benign (19).png and benign (186)_mask.png\n",
      "Loading: benign (190).png and benign (187)_mask.png\n",
      "Loading: benign (191).png and benign (188)_mask.png\n",
      "Loading: benign (192).png and benign (189)_mask.png\n",
      "Loading: benign (193).png and benign (19)_mask.png\n",
      "Loading: benign (194).png and benign (190)_mask.png\n",
      "Loading: benign (195).png and benign (191)_mask.png\n",
      "Loading: benign (196).png and benign (192)_mask.png\n",
      "Loading: benign (197).png and benign (193)_mask.png\n",
      "Loading: benign (198).png and benign (194)_mask.png\n",
      "Loading: benign (199).png and benign (195)_mask.png\n",
      "Loading: benign (2).png and benign (195)_mask_1.png\n",
      "Loading: benign (20).png and benign (195)_mask_2.png\n",
      "Loading: benign (200).png and benign (196)_mask.png\n",
      "Loading: benign (201).png and benign (197)_mask.png\n",
      "Loading: benign (202).png and benign (198)_mask.png\n",
      "Loading: benign (203).png and benign (199)_mask.png\n",
      "Loading: benign (204).png and benign (2)_mask.png\n",
      "Loading: benign (205).png and benign (20)_mask.png\n",
      "Loading: benign (206).png and benign (200)_mask.png\n",
      "Loading: benign (207).png and benign (201)_mask.png\n",
      "Loading: benign (208).png and benign (202)_mask.png\n",
      "Loading: benign (209).png and benign (203)_mask.png\n",
      "Loading: benign (21).png and benign (204)_mask.png\n",
      "Loading: benign (210).png and benign (205)_mask.png\n",
      "Loading: benign (211).png and benign (206)_mask.png\n",
      "Loading: benign (212).png and benign (207)_mask.png\n",
      "Loading: benign (213).png and benign (208)_mask.png\n",
      "Loading: benign (214).png and benign (209)_mask.png\n",
      "Loading: benign (215).png and benign (21)_mask.png\n",
      "Loading: benign (216).png and benign (210)_mask.png\n",
      "Loading: benign (217).png and benign (211)_mask.png\n",
      "Loading: benign (218).png and benign (212)_mask.png\n",
      "Loading: benign (219).png and benign (213)_mask.png\n",
      "Loading: benign (22).png and benign (214)_mask.png\n",
      "Loading: benign (220).png and benign (215)_mask.png\n",
      "Loading: benign (221).png and benign (216)_mask.png\n",
      "Loading: benign (222).png and benign (217)_mask.png\n",
      "Loading: benign (223).png and benign (218)_mask.png\n",
      "Loading: benign (224).png and benign (219)_mask.png\n",
      "Loading: benign (225).png and benign (22)_mask.png\n",
      "Loading: benign (226).png and benign (220)_mask.png\n",
      "Loading: benign (227).png and benign (221)_mask.png\n",
      "Loading: benign (228).png and benign (222)_mask.png\n",
      "Loading: benign (229).png and benign (223)_mask.png\n",
      "Loading: benign (23).png and benign (224)_mask.png\n",
      "Loading: benign (230).png and benign (225)_mask.png\n",
      "Loading: benign (231).png and benign (226)_mask.png\n",
      "Loading: benign (232).png and benign (227)_mask.png\n",
      "Loading: benign (233).png and benign (228)_mask.png\n",
      "Loading: benign (234).png and benign (229)_mask.png\n",
      "Loading: benign (235).png and benign (23)_mask.png\n",
      "Loading: benign (236).png and benign (230)_mask.png\n",
      "Loading: benign (237).png and benign (231)_mask.png\n",
      "Loading: benign (238).png and benign (232)_mask.png\n",
      "Loading: benign (239).png and benign (233)_mask.png\n",
      "Loading: benign (24).png and benign (234)_mask.png\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(dataset_path, img_size=(256, 256)):\n",
    "    images, masks = [], []\n",
    "    categories = ['benign', 'normal', 'malignant']\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "        image_files = sorted([f for f in os.listdir(category_path) if \"mask\" not in f.lower()])\n",
    "        mask_files = sorted([f for f in os.listdir(category_path) if \"mask\" in f.lower()])\n",
    "\n",
    "        for img_file, mask_file in zip(image_files, mask_files):\n",
    "            print(f\"Loading: {img_file} and {mask_file}\")\n",
    "            img_path, mask_path = os.path.join(category_path, img_file), os.path.join(category_path, mask_file)\n",
    "            img, mask = cv.imread(img_path), cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if img is not None and mask is not None:\n",
    "                images.append(cv.resize(img, img_size) / 255.0)\n",
    "                masks.append(cv.resize(mask, img_size) / 255.0)\n",
    "\n",
    "    return np.array(images), np.expand_dims(np.array(masks), axis=-1)\n",
    "\n",
    "data_path = os.path.expanduser(\"/Users/sofiautoft/.cache/kagglehub/datasets/aryashah2k/breast-ultrasound-images-dataset/versions/1/Dataset_BUSI_with_GT\")\n",
    "images, masks = load_images_from_folder(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73441d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create the DeepLabV3+ model\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet34\",        # or 'resnet50' if you want deeper\n",
    "    encoder_weights=\"imagenet\",     # use ImageNet pre-trained weights\n",
    "    in_channels=3,                  # ultrasound images: 3 if RGB, 1 if grayscale\n",
    "    classes=1                       # binary segmentation (tumor vs. background)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_to_multiple_of_16(image):\n",
    "    # Get the current height and width of the image\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    # Compute the padding needed\n",
    "    pad_h = (16 - height % 16) % 16  # Ensure the pad is a multiple of 16\n",
    "    pad_w = (16 - width % 16) % 16   # Ensure the pad is a multiple of 16\n",
    "\n",
    "    # Apply padding to the image\n",
    "    padded_image = F.pad(image, (0, pad_w, 0, pad_h), mode='constant', value=0)\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cebea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class UltrasoundDataset(Dataset):\n",
    "    def __init__(self, image_arrays, mask_arrays, transforms=None):\n",
    "        self.image_arrays = image_arrays\n",
    "        self.mask_arrays = mask_arrays\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_arrays)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_arrays[idx]   # already numpy array\n",
    "        mask = self.mask_arrays[idx]     # already numpy array\n",
    "        mylist = [image, mask]\n",
    "    \n",
    "        for i in range(2):\n",
    "            item = mylist[i]\n",
    "            h, w = item.shape[:2]\n",
    "\n",
    "            # Compute the padding needed\n",
    "            new_h = (h + 15) // 16 * 16  # Next multiple of 16\n",
    "            new_w = (w + 15) // 16 * 16  # Next multiple of 16\n",
    "\n",
    "            # Apply padding to the image or mask\n",
    "            mylist[i] = np.pad(item, ((0, new_h - h), (0, new_w - w), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        image = torch.tensor(mylist[0]).permute(2, 0, 1).float()\n",
    "        mask = torch.tensor(mylist[1]).permute(2, 0, 1).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583534f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5706951",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m model = model.to(device)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Split the dataset into training and validation sets\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "indices = list(range(len(images)))\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_images = [images[i] for i in train_idx]\n",
    "train_masks = [masks[i] for i in train_idx]\n",
    "val_images = [images[i] for i in val_idx]\n",
    "val_masks = [masks[i] for i in val_idx]\n",
    "\n",
    "train_dataset = UltrasoundDataset(train_images, train_masks)\n",
    "val_dataset = UltrasoundDataset(val_images, val_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = loss_fn(outputs, masks.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_score(preds, targets, threshold=0.5):\n",
    "    preds = (preds > threshold).float()\n",
    "    targets = targets.float()\n",
    "    \n",
    "    intersection = (preds * targets).sum(dim=(1,2))  # sum over H and W\n",
    "    union = preds.sum(dim=(1,2)) + targets.sum(dim=(1,2)) - intersection\n",
    "\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)  # avoid divide by zero\n",
    "    return iou.mean()\n",
    "\n",
    "def dice_score(preds, targets, threshold=0.5):\n",
    "    preds = (preds > threshold).float()\n",
    "    targets = targets.float()\n",
    "    \n",
    "    intersection = (preds * targets).sum(dim=(1,2))  # sum over H and W\n",
    "    dice = (2. * intersection + 1e-6) / (preds.sum(dim=(1,2)) + targets.sum(dim=(1,2)) + 1e-6)\n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f646443",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pad(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model.eval()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/classes/DS/liver_segmentation/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/classes/DS/liver_segmentation/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/classes/DS/liver_segmentation/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mUltrasoundDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     15\u001b[39m image = \u001b[38;5;28mself\u001b[39m.image_arrays[idx]   \u001b[38;5;66;03m# already numpy array\u001b[39;00m\n\u001b[32m     16\u001b[39m mask = \u001b[38;5;28mself\u001b[39m.mask_arrays[idx]     \u001b[38;5;66;03m# already numpy array\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m padded_image = \u001b[43mpad_to_multiple_of_16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m padded_mask = pad_to_multiple_of_16(mask)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mpad_to_multiple_of_16\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m      9\u001b[39m pad_w = (\u001b[32m16\u001b[39m - width % \u001b[32m16\u001b[39m) % \u001b[32m16\u001b[39m   \u001b[38;5;66;03m# Ensure the pad is a multiple of 16\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Apply padding to the image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m padded_image = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_h\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconstant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m padded_image\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/classes/DS/liver_segmentation/.venv/lib/python3.13/site-packages/torch/nn/functional.py:5209\u001b[39m, in \u001b[36mpad\u001b[39m\u001b[34m(input, pad, mode, value)\u001b[39m\n\u001b[32m   5202\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   5203\u001b[39m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[32m   5204\u001b[39m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[32m   5205\u001b[39m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[32m   5206\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\n\u001b[32m   5207\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtorch._decomp.decompositions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5208\u001b[39m             )._replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[32m-> \u001b[39m\u001b[32m5209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: pad(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, masks in val_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.squeeze(1)  # remove channel dim\n",
    "\n",
    "        iou = iou_score(outputs, masks)\n",
    "        dice = dice_score(outputs, masks)\n",
    "\n",
    "        print(f\"IoU: {iou.item():.4f}, Dice: {dice.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
